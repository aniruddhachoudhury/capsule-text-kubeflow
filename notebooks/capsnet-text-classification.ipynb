{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "#!pip install -q -U tensorflow-gpu\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_len = 256\n",
    "Routings = 3\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 1000\n",
    "embed_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_text, train_labels), (test_text, test_labels) = tf.keras.datasets.imdb.load_data(num_words=maxlen)\n",
    "train_text = sequence.pad_sequences(train_text, maxlen=maxlen)\n",
    "test_text = sequence.pad_sequences(test_text, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1) (25000, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 40\n",
    "\n",
    "# Cast the labels to floats, needed later\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=maxlen)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "y_train = np.expand_dims(y_train, axis=1)\n",
    "y_test = np.expand_dims(y_test, axis=1)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capsule Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPTED FROM CAPSULE IMPLEMENTATION : https://github.com/bojone/Capsule\n",
    "# Converted to Tensorflow Keras instead of Keras \n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    return scale * x\n",
    "\n",
    "\n",
    "#define our own softmax function instead of K.softmax\n",
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex/K.sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "#A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, share_weights=True, activation='squash', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.share_weights = share_weights\n",
    "        self.activation = squash\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = int(input_shape[-1])\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = int(input_shape[-2])\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = (K.shape(u_vecs)[0])\n",
    "        input_num_capsule = (K.shape(u_vecs)[1])\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        #final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:,:,:,0]) #shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            c = softmax(b, 1)\n",
    "            o = K.batch_dot(c, u_hat_vecs, [2, 2])\n",
    "            if i < self.routings - 1:\n",
    "                o = K.l2_normalize(o, -1)\n",
    "                b = K.batch_dot(o, u_hat_vecs, [2, 3])\n",
    "        return self.activation(o)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Capsule Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(maxlen,))\n",
    "embed_layer = Embedding(max_features,\n",
    "                            embed_size,\n",
    "                            input_length=maxlen)(inputs)\n",
    "embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "x = Bidirectional(GRU(gru_len,\n",
    "                      activation='relu',\n",
    "                      dropout=dropout_p,\n",
    "                      recurrent_dropout=dropout_p,\n",
    "                      return_sequences=True))(embed_layer)\n",
    "capsule = Capsule(num_capsule=Num_capsule,dim_capsule=Dim_capsule,routings=Routings,share_weights=True)(x)\n",
    "\n",
    "capsule = Flatten()(capsule)\n",
    "capsule = Dropout(dropout_p)(capsule)\n",
    "capsule = LeakyReLU()(capsule)\n",
    "\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mirrored Strategy for Multi-GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0\n",
      "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0\n",
      "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0\n",
      "INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:1\n",
      "INFO:tensorflow:Configured nccl all-reduce.\n",
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp8yeeqhw4\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp8yeeqhw4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7fc082749ac8>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc082749fd0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n"
     ]
    }
   ],
   "source": [
    "NUM_GPUS = 2\n",
    "strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS)\n",
    "config = tf.estimator.RunConfig(train_distribute=strategy)\n",
    "estimator = tf.keras.estimator.model_to_estimator(model,\n",
    "                                                  config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Input and Eval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(texts, labels, epochs, batch_size):\n",
    "    # Convert the inputs to a Dataset. (E)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "    # Shuffle, repeat, and batch the examples. (T)\n",
    "    SHUFFLE_SIZE = 5000\n",
    "    #ds = ds.shuffle(SHUFFLE_SIZE).repeat(epochs).batch(batch_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    # Return the dataset. (L)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def eval_fn(texts, labels, epochs, batch_size):\n",
    "    # Convert the inputs to a Dataset. (E)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "    ds = ds.batch(batch_size)\n",
    "    # Return the dataset. (L)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Timing Hook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeHistory(tf.train.SessionRunHook):\n",
    "    def begin(self):\n",
    "        self.times = []\n",
    "    def before_run(self, run_context):\n",
    "        self.iter_time_start = time.time()\n",
    "    def after_run(self, run_context, run_values):\n",
    "        self.times.append(time.time() - self.iter_time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for IndexedSlices.\n",
      "INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/tmp/tmp8yeeqhw4/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: ('/tmp/tmp8yeeqhw4/keras/keras_model.ckpt',)\n",
      "INFO:tensorflow:Warm-starting variable: embedding/embeddings; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: bidirectional/forward_gru/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: bidirectional/forward_gru/recurrent_kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: bidirectional/forward_gru/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: bidirectional/backward_gru/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: bidirectional/backward_gru/recurrent_kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: bidirectional/backward_gru/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense/kernel; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Warm-starting variable: dense/bias; prev_var_name: Unchanged\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp8yeeqhw4/model.ckpt.\n",
      "INFO:tensorflow:Initialize strategy\n",
      "INFO:tensorflow:loss = 0.694301, step = 0\n",
      "INFO:tensorflow:global_step/sec: 0.253328\n",
      "INFO:tensorflow:loss = 0.4095703, step = 100 (394.746 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 151 into /tmp/tmp8yeeqhw4/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.253902\n",
      "INFO:tensorflow:loss = 0.4281932, step = 200 (393.852 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.254677\n",
      "INFO:tensorflow:loss = 0.32299638, step = 300 (392.654 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 304 into /tmp/tmp8yeeqhw4/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 391 into /tmp/tmp8yeeqhw4/model.ckpt.\n",
      "INFO:tensorflow:Finalize strategy.\n",
      "INFO:tensorflow:Loss for final step: 0.2844804.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7fc07552a5c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_hist = TimeHistory()\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "estimator.train(lambda:input_fn(x_train,\n",
    "                                y_train,\n",
    "                                epochs=EPOCHS,\n",
    "                                batch_size=BATCH_SIZE),\n",
    "                hooks=[time_hist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time with 2 GPU(s): 1540.020670890808 seconds\n",
      "16.249132542828235 text samples/second with 2 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "total_time = sum(time_hist.times)\n",
    "print(f\"total time with {NUM_GPUS} GPU(s): {total_time} seconds\")\n",
    "avg_time_per_batch = np.mean(time_hist.times)\n",
    "print(f\"{BATCH_SIZE*NUM_GPUS/avg_time_per_batch} text samples/second with {NUM_GPUS} GPU(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:363: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Starting evaluation at 2019-04-13T17:13:16Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp8yeeqhw4/model.ckpt-391\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-04-13-17:29:56\n",
      "INFO:tensorflow:Saving dict for global step 391: binary_accuracy = 0.86056, global_step = 391, loss = 0.32411215\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 391: /tmp/tmp8yeeqhw4/model.ckpt-391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'binary_accuracy': 0.86056, 'loss': 0.32411215, 'global_step': 391}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(lambda:eval_fn(x_test, \n",
    "                                   y_test,\n",
    "                                   epochs=1,\n",
    "                                   batch_size=BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
